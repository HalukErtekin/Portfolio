{
  "published": true,
  "title": "Handwriting Recognition: Writer Identification and Verification in Handwritten Documents Using Hybrid Deep Learning Techniques",
  "description": "Multi-model AI system that transcribes IAM handwriting samples while fingerprinting each writer via CRNN, Siamese, and boosted hybrid pipelines.",
  "date": "2025-02-17T00:00:00.000Z",
  "body": {
    "raw": "\nimport Image from \"next/image\";\n\n## Overview\n\nThis project analyzes images from the IAM Handwriting Dataset for two objectives at once: transcribing the written words/sentences and identifying who wrote them. CNN, CRNN, SVM, Siamese Networks, and XGBoost configurations all run through the same experimental pipeline, creating a reproducible blueprint for biometric verification, forensic analysis, and intelligent document workflows.\n\n## Project Motivation\n\nConventional handwriting systems only learn _what_ was written. Here the pipeline also models _who_ wrote it by capturing cues like stroke pressure, spacing, slant, and rhythm. Recognition and verification models share the same data path so even with few samples, the system can still attribute authorship.\n\n## Dataset & Preprocessing\n\n- Selected the 49 IAM writers with the most samples, yielding 4,207 single-channel 113Ã—113 images.\n- Applied margin cropping, grayscale normalization, and augmentation (rotation, inversion, noise) to enrich data.\n- One-hot encoded writer labels and used stratified train/validation/test splits to keep classes balanced.\n\n<div className=\"grid gap-6 md:grid-cols-2\">\n\t<div className=\"space-y-2 text-sm text-zinc-400\">\n\t\t<Image\n\t\t\tsrc=\"/projects/handwriting/orijinal.png\"\n\t\t\talt=\"Original IAM word sample\"\n\t\t\twidth={900}\n\t\t\theight={600}\n\t\t\tclassName=\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"\n\t\t/>\n\t\t<p>Raw IAM sample prior to the cleaning pipeline.</p>\n\t</div>\n\t<div className=\"space-y-2 text-sm text-zinc-400\">\n\t\t<Image\n\t\t\tsrc=\"/projects/handwriting/patchlenmis.png\"\n\t\t\talt=\"Patched IAM word sample\"\n\t\t\twidth={900}\n\t\t\theight={600}\n\t\t\tclassName=\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"\n\t\t/>\n\t\t<p>Patched, deskewed, and normalized crop fed to the CNN/CRNN stack.</p>\n\t</div>\n</div>\n\nThe verification problem changes difficulty depending on how distinctive the handwriting is. I therefore curated two qualitative buckets that were used for human-in-the-loop checks:\n\n<div className=\"grid gap-6 md:grid-cols-2\">\n\t<Image\n\t\tsrc=\"/projects/handwriting/karakteristik-yazarlar.png\"\n\t\talt=\"Highly characteristic handwriting samples\"\n\t\twidth={1200}\n\t\theight={900}\n\t\tclassName=\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"\n\t/>\n\t<Image\n\t\tsrc=\"/projects/handwriting/karakteristik-olmayan-yazarlar.png\"\n\t\talt=\"Less characteristic handwriting samples\"\n\t\twidth={1200}\n\t\theight={900}\n\t\tclassName=\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"\n\t/>\n</div>\n\n## Model Architectures\n\n### CNN\n\nExtracts spatial stroke features and delivers ~77% writer accuracy as the baseline classifier.\n\n### CRNN\n\nStacks CNN and BiLSTM layers so the network learns shape, rhythm, and sequence; CTC loss supports fast transcription plus ~88% writer accuracy.\n\n### CNN + SVM\n\nFeeds CNN embeddings into an SVM decision boundary, reaching ~87% accuracy and blending deep features with classical ML for smaller datasets.\n\n### CRNN + SVM\n\nSupplies temporal CRNN features to an SVM classifier for 89.9% accuracy and improved robustness to writer-to-writer variation.\n\n### CRNN + XGBoost\n\nRuns gradient-boosted trees on CRNN feature vectors, capturing complex interactions and delivering the best accuracy at 90.32%.\n\n### Siamese Network\n\nUses weight-sharing CNN twins with contrastive loss to decide whether two samples came from the same writer; ~85% verification accuracy adds a dedicated security layer.\n\n## Multi-Task Learning\n\nShared convolutional and recurrent layers learn both tasks simultaneously:\n\n1. CTC-based handwriting recognition.\n2. Style-driven writer classification.\n\nThis shared feature bank yields richer representations, better training efficiency, and workflows that mirror real forensic pipelines.\n\n### Optimization & Training Stability\n\nThe CRNN variants were run through staged learning-rate windows until convergence. Loss curves stayed smooth thanks to gradient clipping and CTC stabilization:\n\n<div className=\"grid gap-6 md:grid-cols-2\">\n\t<Image\n\t\tsrc=\"/projects/handwriting/training-history1-2-3.png\"\n\t\talt=\"Training history for first three experiments\"\n\t\twidth={1200}\n\t\theight={850}\n\t\tclassName=\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"\n\t/>\n\t<Image\n\t\tsrc=\"/projects/handwriting/training-history4-5.png\"\n\t\talt=\"Training history for final experiments\"\n\t\twidth={1200}\n\t\theight={850}\n\t\tclassName=\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"\n\t/>\n</div>\n\n## Performance Summary\n\n| Model                  | Accuracy | Notes                                             |\n| ---------------------- | -------- | ------------------------------------------------- |\n| CNN                    | 77.70%   | Strong spatial learner, limited temporal modeling |\n| CRNN                   | 88.69%   | Excellent sequence modeling                       |\n| CNN + SVM              | 87.46%   | Deep features plus classical boundary             |\n| CRNN + SVM             | 89.96%   | Temporal features with SVM synergy                |\n| **CRNN + XGBoost**     | **90.32%** | **Best overall performance**                     |\n| Siamese (verification) | 85.35%   | Binary scoring for writer verification            |\n\nEach experiment ships with confusion matrices, per-writer metrics, and probability distribution charts.\n\n<div className=\"grid gap-6 md:grid-cols-2\">\n\t<Image\n\t\tsrc=\"/projects/handwriting/cnn-accuracy.png\"\n\t\talt=\"CNN accuracy per writer\"\n\t\twidth={1200}\n\t\theight={900}\n\t\tclassName=\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"\n\t/>\n\t<Image\n\t\tsrc=\"/projects/handwriting/crnn-accuracy.png\"\n\t\talt=\"CRNN accuracy per writer\"\n\t\twidth={1200}\n\t\theight={900}\n\t\tclassName=\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"\n\t/>\n</div>\n\n<div className=\"mt-6 space-y-2 text-sm text-zinc-400\">\n\t<Image\n\t\tsrc=\"/projects/handwriting/comparison.png\"\n\t\talt=\"Overall comparison between architectures\"\n\t\twidth={1600}\n\t\theight={900}\n\t\tclassName=\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"\n\t/>\n\t<p>Side-by-side benchmark summary that guided the decision to ship the CRNN + XGBoost stack for production writer identification.</p>\n</div>\n\n## Strengths\n\n- Comprehensive comparison of multiple architectures in a single pipeline.\n- Fair evaluation via a balanced IAM subset.\n- Strong hybrid performance from CRNN + XGBoost and CRNN + SVM.\n- Siamese verification layer primes the system for biometric deployments.\n- Visualizations and reporting tailored for forensic stakeholders (examples above).\n\n## Key Contributions\n\n- Trained and compared six models (CNN, CRNN, hybrids, Siamese) under one infrastructure with uniform logging.\n- Introduced a writer-ID approach that feeds CRNN features into XGBoost.\n- Demonstrated dual-task learning where transcription and author ID share the same network.\n- Published the balanced IAM subset pipeline for reproducibility.\n\n## Applications\n\n- Forensic document reviews and signature/check verification.\n- Biometric identity authentication systems.\n- Archive automation and historical handwriting analytics.\n- Intelligent handwriting analysis for banking or contract control.\n\n## Conclusion\n\nThe CRNN + XGBoost combo proves how deep sequential learning and ensemble decisions reinforce each other, while the Siamese verifier supplies the identity-check layer production systems expect. Overall the pipeline offers both scientific rigor and field readiness for handwriting-based biometrics.\n\n## Summary\n\nI built an end-to-end AI pipeline that transcribes IAM handwriting with CTC while learning writer identity, elevates CRNN + XGBoost to peak accuracy, and layers on Siamese verification for additional security.\n\n\n",
    "code": "var Component=(()=>{var nt=Object.create;var W=Object.defineProperty;var it=Object.getOwnPropertyDescriptor;var at=Object.getOwnPropertyNames;var ot=Object.getPrototypeOf,st=Object.prototype.hasOwnProperty;var p=(r,e)=>()=>(e||r((e={exports:{}}).exports,e),e.exports),ct=(r,e)=>{for(var n in e)W(r,n,{get:e[n],enumerable:!0})},ve=(r,e,n,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of at(e))!st.call(r,a)&&a!==n&&W(r,a,{get:()=>e[a],enumerable:!(i=it(e,a))||i.enumerable});return r};var _e=(r,e,n)=>(n=r!=null?nt(ot(r)):{},ve(e||!r||!r.__esModule?W(n,\"default\",{value:r,enumerable:!0}):n,r)),lt=r=>ve(W({},\"__esModule\",{value:!0}),r);var Ie=p((Cr,Ne)=>{Ne.exports=_jsx_runtime});var z=p(H=>{\"use strict\";H._=H._interop_require_default=dt;function dt(r){return r&&r.__esModule?r:{default:r}}});var V=p(X=>{\"use strict\";Object.defineProperty(X,\"__esModule\",{value:!0});Object.defineProperty(X,\"warnOnce\",{enumerable:!0,get:function(){return ut}});var ut=r=>{}});var Se=p(Y=>{\"use strict\";Object.defineProperty(Y,\"__esModule\",{value:!0});Object.defineProperty(Y,\"getImageBlurSvg\",{enumerable:!0,get:function(){return ht}});function ht(r){let{widthInt:e,heightInt:n,blurWidth:i,blurHeight:a,blurDataURL:s,objectFit:o}=r,c=20,l=i?i*40:e,d=a?a*40:n,g=l&&d?\"viewBox='0 0 \"+l+\" \"+d+\"'\":\"\",h=g?\"none\":o===\"contain\"?\"xMidYMid\":o===\"cover\"?\"xMidYMid slice\":\"none\";return\"%3Csvg xmlns='http://www.w3.org/2000/svg' \"+g+\"%3E%3Cfilter id='b' color-interpolation-filters='sRGB'%3E%3CfeGaussianBlur stdDeviation='\"+c+\"'/%3E%3CfeColorMatrix values='1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1' result='s'/%3E%3CfeFlood x='0' y='0' width='100%25' height='100%25'/%3E%3CfeComposite operator='out' in='s'/%3E%3CfeComposite in2='SourceGraphic'/%3E%3CfeGaussianBlur stdDeviation='\"+c+\"'/%3E%3C/filter%3E%3Cimage width='100%25' height='100%25' x='0' y='0' preserveAspectRatio='\"+h+\"' style='filter: url(%23b);' href='\"+s+\"'/%3E%3C/svg%3E\"}});var F=p(K=>{\"use strict\";Object.defineProperty(K,\"__esModule\",{value:!0});function pt(r,e){for(var n in e)Object.defineProperty(r,n,{enumerable:!0,get:e[n]})}pt(K,{VALID_LOADERS:function(){return ft},imageConfigDefault:function(){return gt}});var ft=[\"default\",\"imgix\",\"cloudinary\",\"akamai\",\"custom\"],gt={deviceSizes:[640,750,828,1080,1200,1920,2048,3840],imageSizes:[16,32,48,64,96,128,256,384],path:\"/_next/image\",loader:\"default\",loaderFile:\"\",domains:[],disableStaticImages:!1,minimumCacheTTL:60,formats:[\"image/webp\"],dangerouslyAllowSVG:!1,contentSecurityPolicy:\"script-src 'none'; frame-src 'none'; sandbox;\",contentDispositionType:\"inline\",remotePatterns:[],unoptimized:!1}});var Q=p($=>{\"use strict\";Object.defineProperty($,\"__esModule\",{value:!0});Object.defineProperty($,\"getImgProps\",{enumerable:!0,get:function(){return Nt}});var Pr=V(),mt=Se(),wt=F();function xe(r){return r.default!==void 0}function yt(r){return r.src!==void 0}function bt(r){return typeof r==\"object\"&&(xe(r)||yt(r))}function J(r){return typeof r>\"u\"?r:typeof r==\"number\"?Number.isFinite(r)?r:NaN:typeof r==\"string\"&&/^[0-9]+$/.test(r)?parseInt(r,10):NaN}function vt(r,e,n){let{deviceSizes:i,allSizes:a}=r;if(n){let o=/(^|\\s)(1?\\d?\\d)vw/g,c=[];for(let l;l=o.exec(n);l)c.push(parseInt(l[2]));if(c.length){let l=Math.min(...c)*.01;return{widths:a.filter(d=>d>=i[0]*l),kind:\"w\"}}return{widths:a,kind:\"w\"}}return typeof e!=\"number\"?{widths:i,kind:\"w\"}:{widths:[...new Set([e,e*2].map(o=>a.find(c=>c>=o)||a[a.length-1]))],kind:\"x\"}}function _t(r){let{config:e,src:n,unoptimized:i,width:a,quality:s,sizes:o,loader:c}=r;if(i)return{src:n,srcSet:void 0,sizes:void 0};let{widths:l,kind:d}=vt(e,a,o),g=l.length-1;return{sizes:!o&&d===\"w\"?\"100vw\":o,srcSet:l.map((h,N)=>c({config:e,src:n,quality:s,width:h})+\" \"+(d===\"w\"?h:N+1)+d).join(\", \"),src:c({config:e,src:n,quality:s,width:l[g]})}}function Nt(r,e){let{src:n,sizes:i,unoptimized:a=!1,priority:s=!1,loading:o,className:c,quality:l,width:d,height:g,fill:h=!1,style:N,onLoad:R,onLoadingComplete:T,placeholder:m=\"empty\",blurDataURL:y,fetchPriority:x,layout:M,objectFit:j,objectPosition:I,lazyBoundary:pe,lazyRoot:b,...D}=r,{imgConf:$e,showAltText:Qe,blurComplete:Ze,defaultLoader:et}=e,A,q=$e||wt.imageConfigDefault;if(\"allSizes\"in q)A=q;else{let u=[...q.deviceSizes,...q.imageSizes].sort((E,O)=>E-O),w=q.deviceSizes.sort((E,O)=>E-O);A={...q,allSizes:u,deviceSizes:w}}let U=D.loader||et;delete D.loader,delete D.srcSet;let fe=\"__next_img_default\"in U;if(fe){if(A.loader===\"custom\")throw new Error('Image with src \"'+n+`\" is missing \"loader\" prop.\nRead more: https://nextjs.org/docs/messages/next-image-missing-loader`)}else{let u=U;U=w=>{let{config:E,...O}=w;return u(O)}}if(M){M===\"fill\"&&(h=!0);let u={intrinsic:{maxWidth:\"100%\",height:\"auto\"},responsive:{width:\"100%\",height:\"auto\"}},w={responsive:\"100vw\",fill:\"100vw\"},E=u[M];E&&(N={...N,...E});let O=w[M];O&&!i&&(i=O)}let ge=\"\",C=J(d),P=J(g),me,we;if(bt(n)){let u=xe(n)?n.default:n;if(!u.src)throw new Error(\"An object should only be passed to the image component src parameter if it comes from a static image import. It must include src. Received \"+JSON.stringify(u));if(!u.height||!u.width)throw new Error(\"An object should only be passed to the image component src parameter if it comes from a static image import. It must include height and width. Received \"+JSON.stringify(u));if(me=u.blurWidth,we=u.blurHeight,y=y||u.blurDataURL,ge=u.src,!h){if(!C&&!P)C=u.width,P=u.height;else if(C&&!P){let w=C/u.width;P=Math.round(u.height*w)}else if(!C&&P){let w=P/u.height;C=Math.round(u.width*w)}}}n=typeof n==\"string\"?n:ge;let ye=!s&&(o===\"lazy\"||typeof o>\"u\");(!n||n.startsWith(\"data:\")||n.startsWith(\"blob:\"))&&(a=!0,ye=!1),A.unoptimized&&(a=!0),fe&&n.endsWith(\".svg\")&&!A.dangerouslyAllowSVG&&(a=!0),s&&(x=\"high\");let tt=J(l);if(!1){if(!a&&!fe)try{}catch(E){}if(typeof window<\"u\"&&!perfObserver&&window.PerformanceObserver)try{}catch(u){}}let B=Object.assign(h?{position:\"absolute\",height:\"100%\",width:\"100%\",left:0,top:0,right:0,bottom:0,objectFit:j,objectPosition:I}:{},Qe?{}:{color:\"transparent\"},N),be=!Ze&&m!==\"empty\"?m===\"blur\"?'url(\"data:image/svg+xml;charset=utf-8,'+(0,mt.getImageBlurSvg)({widthInt:C,heightInt:P,blurWidth:me,blurHeight:we,blurDataURL:y||\"\",objectFit:B.objectFit})+'\")':'url(\"'+m+'\")':null,rt=be?{backgroundSize:B.objectFit||\"cover\",backgroundPosition:B.objectPosition||\"50% 50%\",backgroundRepeat:\"no-repeat\",backgroundImage:be}:{},G=_t({config:A,src:n,unoptimized:a,width:C,quality:tt,sizes:i,loader:U});if(!1&&typeof window<\"u\")try{}catch(w){}return{props:{...D,loading:ye?\"lazy\":o,fetchPriority:x,width:C,height:P,decoding:\"async\",className:c,style:{...B,...rt},sizes:G.sizes,srcSet:G.srcSet,src:G.src},meta:{unoptimized:a,priority:s,placeholder:m,fill:h}}}});var ee=p(Z=>{\"use strict\";function Ce(r){if(typeof WeakMap!=\"function\")return null;var e=new WeakMap,n=new WeakMap;return(Ce=function(i){return i?n:e})(r)}Z._=Z._interop_require_wildcard=It;function It(r,e){if(!e&&r&&r.__esModule)return r;if(r===null||typeof r!=\"object\"&&typeof r!=\"function\")return{default:r};var n=Ce(e);if(n&&n.has(r))return n.get(r);var i={},a=Object.defineProperty&&Object.getOwnPropertyDescriptor;for(var s in r)if(s!==\"default\"&&Object.prototype.hasOwnProperty.call(r,s)){var o=a?Object.getOwnPropertyDescriptor(r,s):null;o&&(o.get||o.set)?Object.defineProperty(i,s,o):i[s]=r[s]}return i.default=r,n&&n.set(r,i),i}});var L=p((Rr,Ee)=>{Ee.exports=React});var ke=p((jr,Oe)=>{Oe.exports=ReactDOM});var Pe=p(ne=>{\"use strict\";Object.defineProperty(ne,\"__esModule\",{value:!0});Object.defineProperty(ne,\"default\",{enumerable:!0,get:function(){return xt}});var te=L(),re=typeof window>\"u\",Me=re?()=>{}:te.useLayoutEffect,St=re?()=>{}:te.useEffect;function xt(r){let{headManager:e,reduceComponentsToState:n}=r;function i(){if(e&&e.mountedInstances){let s=te.Children.toArray(Array.from(e.mountedInstances).filter(Boolean));e.updateHead(n(s,r))}}if(re){var a;e==null||(a=e.mountedInstances)==null||a.add(r.children),i()}return Me(()=>{var s;return e==null||(s=e.mountedInstances)==null||s.add(r.children),()=>{var o;e==null||(o=e.mountedInstances)==null||o.delete(r.children)}}),Me(()=>(e&&(e._pendingUpdate=i),()=>{e&&(e._pendingUpdate=i)})),St(()=>(e&&e._pendingUpdate&&(e._pendingUpdate(),e._pendingUpdate=null),()=>{e&&e._pendingUpdate&&(e._pendingUpdate(),e._pendingUpdate=null)})),null}});var ze=p(ie=>{\"use strict\";Object.defineProperty(ie,\"__esModule\",{value:!0});Object.defineProperty(ie,\"AmpStateContext\",{enumerable:!0,get:function(){return Ot}});var Ct=z(),Et=Ct._(L()),Ot=Et.default.createContext({})});var Le=p(ae=>{\"use strict\";Object.defineProperty(ae,\"__esModule\",{value:!0});Object.defineProperty(ae,\"HeadManagerContext\",{enumerable:!0,get:function(){return Pt}});var kt=z(),Mt=kt._(L()),Pt=Mt.default.createContext({})});var Re=p(oe=>{\"use strict\";Object.defineProperty(oe,\"__esModule\",{value:!0});Object.defineProperty(oe,\"isInAmpMode\",{enumerable:!0,get:function(){return zt}});function zt(r){let{ampFirst:e=!1,hybrid:n=!1,hasQuery:i=!1}=r===void 0?{}:r;return e||n&&i}});var qe=p((v,Ae)=>{\"use client\";\"use strict\";Object.defineProperty(v,\"__esModule\",{value:!0});function Lt(r,e){for(var n in e)Object.defineProperty(r,n,{enumerable:!0,get:e[n]})}Lt(v,{defaultHead:function(){return De},default:function(){return Ft}});var Rt=z(),jt=ee(),k=jt._(L()),Dt=Rt._(Pe()),At=ze(),qt=Le(),Tt=Re(),Vr=V();function De(r){r===void 0&&(r=!1);let e=[k.default.createElement(\"meta\",{charSet:\"utf-8\"})];return r||e.push(k.default.createElement(\"meta\",{name:\"viewport\",content:\"width=device-width\"})),e}function Vt(r,e){return typeof e==\"string\"||typeof e==\"number\"?r:e.type===k.default.Fragment?r.concat(k.default.Children.toArray(e.props.children).reduce((n,i)=>typeof i==\"string\"||typeof i==\"number\"?n:n.concat(i),[])):r.concat(e)}var je=[\"name\",\"httpEquiv\",\"charSet\",\"itemProp\"];function Ut(){let r=new Set,e=new Set,n=new Set,i={};return a=>{let s=!0,o=!1;if(a.key&&typeof a.key!=\"number\"&&a.key.indexOf(\"$\")>0){o=!0;let c=a.key.slice(a.key.indexOf(\"$\")+1);r.has(c)?s=!1:r.add(c)}switch(a.type){case\"title\":case\"base\":e.has(a.type)?s=!1:e.add(a.type);break;case\"meta\":for(let c=0,l=je.length;c<l;c++){let d=je[c];if(a.props.hasOwnProperty(d))if(d===\"charSet\")n.has(d)?s=!1:n.add(d);else{let g=a.props[d],h=i[d]||new Set;(d!==\"name\"||!o)&&h.has(g)?s=!1:(h.add(g),i[d]=h)}}break}return s}}function Bt(r,e){let{inAmpMode:n}=e;return r.reduce(Vt,[]).reverse().concat(De(n).reverse()).filter(Ut()).reverse().map((i,a)=>{let s=i.key||a;if(process.env.__NEXT_OPTIMIZE_FONTS&&!n&&i.type===\"link\"&&i.props.href&&[\"https://fonts.googleapis.com/css\",\"https://use.typekit.net/\"].some(o=>i.props.href.startsWith(o))){let o={...i.props||{}};return o[\"data-href\"]=o.href,o.href=void 0,o[\"data-optimized-fonts\"]=!0,k.default.cloneElement(i,o)}return k.default.cloneElement(i,{key:s})})}function Wt(r){let{children:e}=r,n=(0,k.useContext)(At.AmpStateContext),i=(0,k.useContext)(qt.HeadManagerContext);return k.default.createElement(Dt.default,{reduceComponentsToState:Bt,headManager:i,inAmpMode:(0,Tt.isInAmpMode)(n)},e)}var Ft=Wt;(typeof v.default==\"function\"||typeof v.default==\"object\"&&v.default!==null)&&typeof v.default.__esModule>\"u\"&&(Object.defineProperty(v.default,\"__esModule\",{value:!0}),Object.assign(v.default,v),Ae.exports=v.default)});var Te=p(se=>{\"use strict\";Object.defineProperty(se,\"__esModule\",{value:!0});Object.defineProperty(se,\"ImageConfigContext\",{enumerable:!0,get:function(){return Yt}});var Gt=z(),Ht=Gt._(L()),Xt=F(),Yt=Ht.default.createContext(Xt.imageConfigDefault)});var Ve=p(ce=>{\"use strict\";Object.defineProperty(ce,\"__esModule\",{value:!0});Object.defineProperty(ce,\"RouterContext\",{enumerable:!0,get:function(){return $t}});var Kt=z(),Jt=Kt._(L()),$t=Jt.default.createContext(null)});var de=p(le=>{\"use strict\";Object.defineProperty(le,\"__esModule\",{value:!0});Object.defineProperty(le,\"default\",{enumerable:!0,get:function(){return Qt}});function Ue(r){let{config:e,src:n,width:i,quality:a}=r;if(!1&&!n.startsWith(\"/\")&&(e.domains||e.remotePatterns))try{}catch(c){}return e.path+\"?url=\"+encodeURIComponent(n)+\"&w=\"+i+\"&q=\"+(a||75)+(process.env.NEXT_DEPLOYMENT_ID?\"&dpl=\"+process.env.NEXT_DEPLOYMENT_ID:\"\")}Ue.__next_img_default=!0;var Qt=Ue});var He=p((_,Ge)=>{\"use client\";\"use strict\";Object.defineProperty(_,\"__esModule\",{value:!0});Object.defineProperty(_,\"Image\",{enumerable:!0,get:function(){return lr}});var ue=z(),Zt=ee(),f=Zt._(L()),Be=ue._(ke()),er=ue._(qe()),tr=Q(),rr=F(),nr=Te(),Fr=V(),ir=Ve(),ar=ue._(de()),or=process.env.__NEXT_IMAGE_OPTS;typeof window>\"u\"&&(globalThis.__NEXT_IMAGE_IMPORTED=!0);function We(r,e,n,i,a,s){let o=r?.src;if(!r||r[\"data-loaded-src\"]===o)return;r[\"data-loaded-src\"]=o,(\"decode\"in r?r.decode():Promise.resolve()).catch(()=>{}).then(()=>{if(!(!r.parentElement||!r.isConnected)){if(e!==\"empty\"&&a(!0),n?.current){let l=new Event(\"load\");Object.defineProperty(l,\"target\",{writable:!1,value:r});let d=!1,g=!1;n.current({...l,nativeEvent:l,currentTarget:r,target:r,isDefaultPrevented:()=>d,isPropagationStopped:()=>g,persist:()=>{},preventDefault:()=>{d=!0,l.preventDefault()},stopPropagation:()=>{g=!0,l.stopPropagation()}})}i?.current&&i.current(r)}})}function Fe(r){let[e,n]=f.version.split(\".\"),i=parseInt(e,10),a=parseInt(n,10);return i>18||i===18&&a>=3?{fetchPriority:r}:{fetchpriority:r}}var sr=(0,f.forwardRef)((r,e)=>{let{src:n,srcSet:i,sizes:a,height:s,width:o,decoding:c,className:l,style:d,fetchPriority:g,placeholder:h,loading:N,unoptimized:R,fill:T,onLoadRef:m,onLoadingCompleteRef:y,setBlurComplete:x,setShowAltText:M,onLoad:j,onError:I,...pe}=r;return f.default.createElement(\"img\",{...pe,...Fe(g),loading:N,width:o,height:s,decoding:c,\"data-nimg\":T?\"fill\":\"1\",className:l,style:d,sizes:a,srcSet:i,src:n,ref:(0,f.useCallback)(b=>{e&&(typeof e==\"function\"?e(b):typeof e==\"object\"&&(e.current=b)),b&&(I&&(b.src=b.src),b.complete&&We(b,h,m,y,x,R))},[n,h,m,y,x,I,R,e]),onLoad:b=>{let D=b.currentTarget;We(D,h,m,y,x,R)},onError:b=>{M(!0),h!==\"empty\"&&x(!0),I&&I(b)}})});function cr(r){let{isAppRouter:e,imgAttributes:n}=r,i={as:\"image\",imageSrcSet:n.srcSet,imageSizes:n.sizes,crossOrigin:n.crossOrigin,referrerPolicy:n.referrerPolicy,...Fe(n.fetchPriority)};return e&&Be.default.preload?(Be.default.preload(n.src,i),null):f.default.createElement(er.default,null,f.default.createElement(\"link\",{key:\"__nimg-\"+n.src+n.srcSet+n.sizes,rel:\"preload\",href:n.srcSet?void 0:n.src,...i}))}var lr=(0,f.forwardRef)((r,e)=>{let i=!(0,f.useContext)(ir.RouterContext),a=(0,f.useContext)(nr.ImageConfigContext),s=(0,f.useMemo)(()=>{let y=or||a||rr.imageConfigDefault,x=[...y.deviceSizes,...y.imageSizes].sort((j,I)=>j-I),M=y.deviceSizes.sort((j,I)=>j-I);return{...y,allSizes:x,deviceSizes:M}},[a]),{onLoad:o,onLoadingComplete:c}=r,l=(0,f.useRef)(o);(0,f.useEffect)(()=>{l.current=o},[o]);let d=(0,f.useRef)(c);(0,f.useEffect)(()=>{d.current=c},[c]);let[g,h]=(0,f.useState)(!1),[N,R]=(0,f.useState)(!1),{props:T,meta:m}=(0,tr.getImgProps)(r,{defaultLoader:ar.default,imgConf:s,blurComplete:g,showAltText:N});return f.default.createElement(f.default.Fragment,null,f.default.createElement(sr,{...T,unoptimized:m.unoptimized,placeholder:m.placeholder,fill:m.fill,onLoadRef:l,onLoadingCompleteRef:d,setBlurComplete:h,setShowAltText:R,ref:e}),m.priority?f.default.createElement(cr,{isAppRouter:i,imgAttributes:T}):null)});(typeof _.default==\"function\"||typeof _.default==\"object\"&&_.default!==null)&&typeof _.default.__esModule>\"u\"&&(Object.defineProperty(_.default,\"__esModule\",{value:!0}),Object.assign(_.default,_),Ge.exports=_.default)});var Xe=p(he=>{\"use strict\";Object.defineProperty(he,\"__esModule\",{value:!0});function dr(r,e){for(var n in e)Object.defineProperty(r,n,{enumerable:!0,get:e[n]})}dr(he,{unstable_getImgProps:function(){return mr},default:function(){return wr}});var ur=z(),hr=Q(),pr=V(),fr=He(),gr=ur._(de()),mr=r=>{(0,pr.warnOnce)(\"Warning: unstable_getImgProps() is experimental and may change or be removed at any time. Use at your own risk.\");let{props:e}=(0,hr.getImgProps)(r,{defaultLoader:gr.default,imgConf:process.env.__NEXT_IMAGE_OPTS});for(let[n,i]of Object.entries(e))i===void 0&&delete e[n];return{props:e}},wr=fr.Image});var Ke=p((Hr,Ye)=>{Ye.exports=Xe()});var _r={};ct(_r,{default:()=>vr,frontmatter:()=>yr});var t=_e(Ie()),S=_e(Ke()),yr={title:\"Handwriting Recognition: Writer Identification and Verification in Handwritten Documents Using Hybrid Deep Learning Techniques\",description:\"Multi-model AI system that transcribes IAM handwriting samples while fingerprinting each writer via CRNN, Siamese, and boosted hybrid pipelines.\",date:\"2025-02-17\",published:!0};function Je(r){let e=Object.assign({h2:\"h2\",a:\"a\",span:\"span\",p:\"p\",em:\"em\",ul:\"ul\",li:\"li\",h3:\"h3\",ol:\"ol\",table:\"table\",thead:\"thead\",tr:\"tr\",th:\"th\",tbody:\"tbody\",td:\"td\",strong:\"strong\"},r.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.h2,{id:\"overview\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#overview\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Overview\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This project analyzes images from the IAM Handwriting Dataset for two objectives at once: transcribing the written words/sentences and identifying who wrote them. CNN, CRNN, SVM, Siamese Networks, and XGBoost configurations all run through the same experimental pipeline, creating a reproducible blueprint for biometric verification, forensic analysis, and intelligent document workflows.\"}),`\n`,(0,t.jsxs)(e.h2,{id:\"project-motivation\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#project-motivation\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Project Motivation\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Conventional handwriting systems only learn \",(0,t.jsx)(e.em,{children:\"what\"}),\" was written. Here the pipeline also models \",(0,t.jsx)(e.em,{children:\"who\"}),\" wrote it by capturing cues like stroke pressure, spacing, slant, and rhythm. Recognition and verification models share the same data path so even with few samples, the system can still attribute authorship.\"]}),`\n`,(0,t.jsxs)(e.h2,{id:\"dataset--preprocessing\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#dataset--preprocessing\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Dataset & Preprocessing\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Selected the 49 IAM writers with the most samples, yielding 4,207 single-channel 113\\xD7113 images.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Applied margin cropping, grayscale normalization, and augmentation (rotation, inversion, noise) to enrich data.\"}),`\n`,(0,t.jsx)(e.li,{children:\"One-hot encoded writer labels and used stratified train/validation/test splits to keep classes balanced.\"}),`\n`]}),`\n`,(0,t.jsxs)(\"div\",{className:\"grid gap-6 md:grid-cols-2\",children:[(0,t.jsxs)(\"div\",{className:\"space-y-2 text-sm text-zinc-400\",children:[(0,t.jsx)(S.default,{src:\"/projects/handwriting/orijinal.png\",alt:\"Original IAM word sample\",width:900,height:600,className:\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"}),(0,t.jsx)(\"p\",{children:\"Raw IAM sample prior to the cleaning pipeline.\"})]}),(0,t.jsxs)(\"div\",{className:\"space-y-2 text-sm text-zinc-400\",children:[(0,t.jsx)(S.default,{src:\"/projects/handwriting/patchlenmis.png\",alt:\"Patched IAM word sample\",width:900,height:600,className:\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"}),(0,t.jsx)(\"p\",{children:\"Patched, deskewed, and normalized crop fed to the CNN/CRNN stack.\"})]})]}),`\n`,(0,t.jsx)(e.p,{children:\"The verification problem changes difficulty depending on how distinctive the handwriting is. I therefore curated two qualitative buckets that were used for human-in-the-loop checks:\"}),`\n`,(0,t.jsxs)(\"div\",{className:\"grid gap-6 md:grid-cols-2\",children:[(0,t.jsx)(S.default,{src:\"/projects/handwriting/karakteristik-yazarlar.png\",alt:\"Highly characteristic handwriting samples\",width:1200,height:900,className:\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"}),(0,t.jsx)(S.default,{src:\"/projects/handwriting/karakteristik-olmayan-yazarlar.png\",alt:\"Less characteristic handwriting samples\",width:1200,height:900,className:\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"})]}),`\n`,(0,t.jsxs)(e.h2,{id:\"model-architectures\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#model-architectures\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Model Architectures\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"cnn\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#cnn\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"CNN\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Extracts spatial stroke features and delivers ~77% writer accuracy as the baseline classifier.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"crnn\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#crnn\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"CRNN\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Stacks CNN and BiLSTM layers so the network learns shape, rhythm, and sequence; CTC loss supports fast transcription plus ~88% writer accuracy.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"cnn--svm\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#cnn--svm\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"CNN + SVM\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Feeds CNN embeddings into an SVM decision boundary, reaching ~87% accuracy and blending deep features with classical ML for smaller datasets.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"crnn--svm\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#crnn--svm\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"CRNN + SVM\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Supplies temporal CRNN features to an SVM classifier for 89.9% accuracy and improved robustness to writer-to-writer variation.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"crnn--xgboost\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#crnn--xgboost\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"CRNN + XGBoost\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Runs gradient-boosted trees on CRNN feature vectors, capturing complex interactions and delivering the best accuracy at 90.32%.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"siamese-network\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#siamese-network\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Siamese Network\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Uses weight-sharing CNN twins with contrastive loss to decide whether two samples came from the same writer; ~85% verification accuracy adds a dedicated security layer.\"}),`\n`,(0,t.jsxs)(e.h2,{id:\"multi-task-learning\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#multi-task-learning\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Multi-Task Learning\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Shared convolutional and recurrent layers learn both tasks simultaneously:\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsx)(e.li,{children:\"CTC-based handwriting recognition.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Style-driven writer classification.\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"This shared feature bank yields richer representations, better training efficiency, and workflows that mirror real forensic pipelines.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"optimization--training-stability\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#optimization--training-stability\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Optimization & Training Stability\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The CRNN variants were run through staged learning-rate windows until convergence. Loss curves stayed smooth thanks to gradient clipping and CTC stabilization:\"}),`\n`,(0,t.jsxs)(\"div\",{className:\"grid gap-6 md:grid-cols-2\",children:[(0,t.jsx)(S.default,{src:\"/projects/handwriting/training-history1-2-3.png\",alt:\"Training history for first three experiments\",width:1200,height:850,className:\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"}),(0,t.jsx)(S.default,{src:\"/projects/handwriting/training-history4-5.png\",alt:\"Training history for final experiments\",width:1200,height:850,className:\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"})]}),`\n`,(0,t.jsxs)(e.h2,{id:\"performance-summary\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#performance-summary\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Performance Summary\"]}),`\n`,(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:\"Model\"}),(0,t.jsx)(e.th,{children:\"Accuracy\"}),(0,t.jsx)(e.th,{children:\"Notes\"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:\"CNN\"}),(0,t.jsx)(e.td,{children:\"77.70%\"}),(0,t.jsx)(e.td,{children:\"Strong spatial learner, limited temporal modeling\"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:\"CRNN\"}),(0,t.jsx)(e.td,{children:\"88.69%\"}),(0,t.jsx)(e.td,{children:\"Excellent sequence modeling\"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:\"CNN + SVM\"}),(0,t.jsx)(e.td,{children:\"87.46%\"}),(0,t.jsx)(e.td,{children:\"Deep features plus classical boundary\"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:\"CRNN + SVM\"}),(0,t.jsx)(e.td,{children:\"89.96%\"}),(0,t.jsx)(e.td,{children:\"Temporal features with SVM synergy\"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:\"CRNN + XGBoost\"})}),(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:\"90.32%\"})}),(0,t.jsx)(e.td,{children:(0,t.jsx)(e.strong,{children:\"Best overall performance\"})})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:\"Siamese (verification)\"}),(0,t.jsx)(e.td,{children:\"85.35%\"}),(0,t.jsx)(e.td,{children:\"Binary scoring for writer verification\"})]})]})]}),`\n`,(0,t.jsx)(e.p,{children:\"Each experiment ships with confusion matrices, per-writer metrics, and probability distribution charts.\"}),`\n`,(0,t.jsxs)(\"div\",{className:\"grid gap-6 md:grid-cols-2\",children:[(0,t.jsx)(S.default,{src:\"/projects/handwriting/cnn-accuracy.png\",alt:\"CNN accuracy per writer\",width:1200,height:900,className:\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"}),(0,t.jsx)(S.default,{src:\"/projects/handwriting/crnn-accuracy.png\",alt:\"CRNN accuracy per writer\",width:1200,height:900,className:\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"})]}),`\n`,(0,t.jsxs)(\"div\",{className:\"mt-6 space-y-2 text-sm text-zinc-400\",children:[(0,t.jsx)(S.default,{src:\"/projects/handwriting/comparison.png\",alt:\"Overall comparison between architectures\",width:1600,height:900,className:\"w-full rounded-2xl border border-zinc-800 bg-zinc-900\"}),(0,t.jsx)(\"p\",{children:\"Side-by-side benchmark summary that guided the decision to ship the CRNN + XGBoost stack for production writer identification.\"})]}),`\n`,(0,t.jsxs)(e.h2,{id:\"strengths\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#strengths\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Strengths\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Comprehensive comparison of multiple architectures in a single pipeline.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Fair evaluation via a balanced IAM subset.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Strong hybrid performance from CRNN + XGBoost and CRNN + SVM.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Siamese verification layer primes the system for biometric deployments.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Visualizations and reporting tailored for forensic stakeholders (examples above).\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h2,{id:\"key-contributions\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#key-contributions\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Key Contributions\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Trained and compared six models (CNN, CRNN, hybrids, Siamese) under one infrastructure with uniform logging.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Introduced a writer-ID approach that feeds CRNN features into XGBoost.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Demonstrated dual-task learning where transcription and author ID share the same network.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Published the balanced IAM subset pipeline for reproducibility.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h2,{id:\"applications\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#applications\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Applications\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Forensic document reviews and signature/check verification.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Biometric identity authentication systems.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Archive automation and historical handwriting analytics.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Intelligent handwriting analysis for banking or contract control.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h2,{id:\"conclusion\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#conclusion\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The CRNN + XGBoost combo proves how deep sequential learning and ensemble decisions reinforce each other, while the Siamese verifier supplies the identity-check layer production systems expect. Overall the pipeline offers both scientific rigor and field readiness for handwriting-based biometrics.\"}),`\n`,(0,t.jsxs)(e.h2,{id:\"summary\",children:[(0,t.jsx)(e.a,{className:\"subheading-anchor\",\"aria-label\":\"Link to section\",href:\"#summary\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,t.jsx)(e.p,{children:\"I built an end-to-end AI pipeline that transcribes IAM handwriting with CTC while learning writer identity, elevates CRNN + XGBoost to peak accuracy, and layers on Siamese verification for additional security.\"})]})}function br(r={}){let{wrapper:e}=r.components||{};return e?(0,t.jsx)(e,Object.assign({},r,{children:(0,t.jsx)(Je,r)})):Je(r)}var vr=br;return lt(_r);})();\n;return Component;"
  },
  "_id": "projects/handwriting-recognition-writer-identification.mdx",
  "_raw": {
    "sourceFilePath": "projects/handwriting-recognition-writer-identification.mdx",
    "sourceFileName": "handwriting-recognition-writer-identification.mdx",
    "sourceFileDir": "projects",
    "contentType": "mdx",
    "flattenedPath": "projects/handwriting-recognition-writer-identification"
  },
  "type": "Project",
  "path": "/projects/handwriting-recognition-writer-identification",
  "slug": "handwriting-recognition-writer-identification"
}